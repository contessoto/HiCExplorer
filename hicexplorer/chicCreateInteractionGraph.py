import argparse
import sys
import errno
import os
import math
from multiprocessing import Process, Queue
from scipy.sparse import coo_matrix, dia_matrix, lil_matrix

import time
import logging
log = logging.getLogger(__name__)

import numpy as np
from scipy import stats
import h5py
import networkx as nx
from holoviews.plotting.util import process_cmap
import matplotlib.pyplot as plt
import io
import tarfile
from contextlib import closing
from pyvis.network import Network

import hicmatrix.HiCMatrix as hm
from intervaltree import IntervalTree, Interval

from hicexplorer import utilities
from hicexplorer._version import __version__
from .lib import Viewpoint


def parse_arguments(args=None):
    parser = argparse.ArgumentParser(add_help=False,
                                     formatter_class=argparse.RawDescriptionHelpFormatter,
                                     description="""
chicCreateInteractionGraph computes the network of a reference point with its regions of interest. To add a region to the graph, a second file containing the chromatin status and peak is required.
For each interaction it is checked if it is matching the position of a peak; moreover, the chromatin status is used.
"""
                                     )

    parserRequired = parser.add_argument_group('Required arguments')

    parserRequired.add_argument('--significantFile', '-af',
                                help='path to the interaction data. Please use only the significant contacts as generated by chicSignificant contacts.',
                                required=True)

    parserRequired.add_argument('--peakFile', '-tf',
                                help='Peak file containing the peaks and chromatin status based on ATAC-Seq data.'
                                )
    # parserRequired.add_argument('--referencePoints', '-rp', help='Reference point file. Needs to be in the format: \'chr 100\' for a '
    #                             'single reference point or \'chr 100 200\' for a reference region and with a single reference point per line',
    #                             required=True)

    parserOpt = parser.add_argument_group('Optional arguments')

    parserOpt.add_argument('--outFileName', '-o',
                           help='Output file for the graph file'
                           ' (Default: %(default)s).',
                           required=False,
                           default='interactionGraph.hdf5')
    parserOpt.add_argument('--plotsFileName', '-po',
                           help='Output tar.gz of the plotted graph files.'
                           ' (Default: %(default)s).',
                           required=False,
                           default='plotInteractionGraphs.tar.gz')
    parserOpt.add_argument('--outputFormat', '-format',
                           help='Output format of the plot'
                           ' (Default: %(default)s).',
                           required=False,
                           default='png')
    parserOpt.add_argument('--threads', '-t',
                           help='Number of threads (uses the python multiprocessing module)'
                           ' (Default: %(default)s).',
                           required=False,
                           default=4,
                           type=int
                           )
    parserOpt.add_argument("--help", "-h", action="help",
                           help="show this help message and exit")
    parserOpt.add_argument('--version', action='version',
                           version='%(prog)s {}'.format(__version__))
    return parser

def readPeaksChromatinFile(pPeakFile):
    # excepted format: peak_id\tchr\tstart\end\tstatus
    
    data = []
    data_assoziation_status_dict = {}
    peak_list = []
    peak_types = []
    with open(pPeakFile, 'r') as file:
        for line in file.readlines():
            _line = line.strip().split('\t')
            data_for_interval = (_line[3], _line[4])
            _data = [_line[0], int(_line[1]), int(_line[2]) ]
            data_assoziation_status_dict[_line[0] + ':' + _line[1] + '-' +  _line[2]] = data_for_interval
            data.append(_data)
            peak_list.append(_line[4])
            if len(data) == 1:
                # log.debug(data)
                tmp_data = _data
            if _line[3] not in peak_types:
                peak_types.append(_line[3])
        hicmatrix = hm.hiCMatrix()
        data_intervaltree = hicmatrix.intervalListToIntervalTree(data)[0]
        # log.debug('tmp_data {}'.format(tmp_data))
        # log.debug('data {}'.format(data))

        # log.debug('data_intervaltree {}'.format(data_intervaltree))

        # log.debug('data_intervaltree {}'.format(data_intervaltree[tmp_data[0]].overlap(tmp_data[1], tmp_data[2])))
    log.debug('{}'.format(peak_types))
    return data_intervaltree, data_assoziation_status_dict, peak_list, peak_types

def computeGraph(pFileList, pPeakDataIntervalTree, pPeaksMetadataDict, pViewpointObject, pArgs, pQueue=None):

    try:
        reference_point_graph = {}
        categories = set()
        images_array = []
        file_names = []
        for j, file in enumerate(pFileList):
            # if j == 5:
            #     break
            for i, sample in enumerate(file):
                # log.debug('sample {}'.format(sample))
                data = pViewpointObject.readInteractionFile(pArgs.significantFile, sample)
                key_list = sorted(list(data[1].keys()))
                collect_connections = []
                for key in key_list:

                    chromosome = data[1][key][0]
                    start = int(data[1][key][1])
                    end = int(data[1][key][2])

                    if chromosome in pPeakDataIntervalTree:
                        peakData = pPeakDataIntervalTree[chromosome].overlap(start, end)
                    else:
                        continue
                    if len(peakData) == 0:
                        continue
                    else:
                        for peak in sorted(peakData):
                            peak_key = chromosome + ':' + str(peak[0]) + '-' + str(peak[1])
                            if peak_key in pPeaksMetadataDict:
                                collect_connections.append([chromosome, list(peak), data[1][key][9],  pPeaksMetadataDict[peak_key]])
                            else:
                                log.warning('Something went wrong. A peak has no metadata!')
                            categories.add(pPeaksMetadataDict[peak_key][0])
                # log.debug('collect_connections {}'.format(collect_connections))
                sample_key = '_'.join(sample)
                reference_point_graph[sample_key] = collect_connections
            # if j > 10:
            #     break
            # break;
        # log.debug('{} graphs are computed, plotting now'.format(len(reference_point_graph)))
        color_map = 'Set1'
        colors = list(set(sorted(process_cmap(color_map))))
        categories.add('RP')
        categories = sorted(list(categories))
        number_of_categories = len(categories)
        color_list = {}
        for i in range(number_of_categories):
            color_list[categories[i]] = i
        # log.debug('colors {}'.format(colors))
        # exit(1)
        for i, graph in enumerate(reference_point_graph):
            # log.debug('graphs {} is plotted'.format(i))

            # log.debug('reference_point_graph[graph] {}'.format(reference_point_graph[graph]))
            G = nx.Graph()
            sample_id = graph.split("_")[3]
            node_list_color = {}
            node_list_color['RP'] = [sample_id]
            edge_weights = []
            # node_list_color_category = {}
            # node_list_color_category['RP'] = 0
            for node in reference_point_graph[graph]:
                G.add_weighted_edges_from([(sample_id, node[3][1] , node[2])])
                G.nodes[node[3][1]]['title'] = node[3][1]
                G.nodes[node[3][1]]['group'] = node[3][0]
                if node[3][0] in node_list_color:
                    node_list_color[node[3][0]].append(node[3][1])
                else:
                    node_list_color[node[3][0]] = [node[3][1]]
                edge_weights.append(node[2])
            if len(edge_weights) == 0:
                continue
            pos = nx.spring_layout(G, seed=0)
            # edge_weights = np.array(edge_weights)
            # max_edge_weight = max(edge_weights)
            # edge_weights /= max_edge_weight
                # node_list_color[node[3][1]] = node[3][0]
            # nx.draw(G,  with_labels=True, nodelist=list(node_list_color.keys()), 
            # node_color=list(node_list_color.values()),
            # cmap= color_map) ## node_color=color_map,
            # log.debug("G nodes {}".format(G.nodes))
            # G.nodes[1]['title'] = 'Number 1'
            # G.nodes[1]['group'] = 1

            # for node_color, nodelist in node_list_color.items():
            #     nx.draw_networkx_nodes(G,  pos, nodelist=nodelist, node_color=colors[color_list[node_color]])

            # labels = {x: x for x in G.nodes}
            # nx.draw_networkx_labels(G, pos, labels, font_size=16, font_color='b')
            # nx.draw_networkx_edges(G, pos, width=edge_weights)
            # nx.draw_circular(G)
            # Add Legend Nodes
            node_groups = list(node_list_color.keys())
            num_actual_nodes = len(reference_point_graph[graph])
            num_legend_nodes = len(node_list_color)
            step = 50
            x = -300
            y = -250
            legend_nodes = [
                (
                    num_actual_nodes + legend_node, 
                    {
                        'group': node_groups[legend_node], 
                        'label': str(node_groups[legend_node]),
                        'size': 30, 
                        # 'fixed': True, # So that we can move the legend nodes around to arrange them better
                        'physics': False, 
                        'x': x, 
                        'y': f'{y + legend_node*step}px',
                        'shape': 'box', 
                        'widthConstraint': 50, 
                        'font': {'size': 20}
                    }
                )
                for legend_node in range(num_legend_nodes)
            ]
            G.add_nodes_from(legend_nodes)

            # plt.tight_layout() 
            nt = Network('500px', '500px')
            nt.from_nx(G)
            # bufferObject = io.BytesIO()
            try:
                bufferObject = io.StringIO()

                nt.save_graph(bufferObject)
            except TypeError as exp:
                # string = io.StringIO()
                # string.write(str(nt.html))
                images_array.append(nt.html.encode('utf8'))

            # log.debug(images_array)
            # nt.save_graph(graph + '.html')
            # images_array.append()
            # plt.close()
            file_names.append(graph)
    except Exception as exp:
        pQueue.put('Fail: ' + str(exp) + traceback.format_exc())
        return
    if pQueue is None:
        return
    pQueue.put([images_array, file_names, reference_point_graph])
    return
    # return 

def writeGraphHDF(pOutFileName, pReferencePointGraph, pPeakList, pPeakTypesList):
    # hdf5 file design
    # matrix_name / chr / gene name
    #  - reference point: start end
    #  - connection list start: start ids
    #  - connection list end: end ids
    #  - classification list
    #  - interaction strength list
    #  - peak name list
    resultFileH5Object = h5py.File(pOutFileName, 'w')
    resultFileH5Object.attrs['type'] = "graph"
    resultFileH5Object.attrs['version'] = __version__

    peak_dict = {}
    reference_point_dict = {}
    peak_type_dict = {}
    for i, peak in enumerate(pPeakList):
        peak_dict[peak] = i 
    for i, graph in enumerate(pReferencePointGraph):
        reference_point_dict[graph] = i
    for i, peak_type in enumerate(pPeakTypesList):
        peak_type_dict[peak_type] = i + 1
    lil_matrix_interaction_data = lil_matrix((len(reference_point_dict), len(peak_dict)))
    lil_matrix_state = lil_matrix((len(reference_point_dict), len(peak_dict)))

    for i, graph in enumerate(pReferencePointGraph):
        # log.debug("pReferencePointGraph {}".format(pReferencePointGraph[graph]))

        graph_id = reference_point_dict[graph]

        for peak in pReferencePointGraph[graph]:
            # log.debug('peak_id {}'.format(peak))
            
            peak_id = peak_dict[peak[3][1]] 
            # log.debug("pReferencePointGraph {}".format(graph))
            # log.debug("pReferencePointGraph {}".format(pReferencePointGraph[graph]))
            # log.debug('peak_id {}'.format(peak_id))
            # log.debug('graph_id {}'.format(graph_id))
            lil_matrix_interaction_data[graph_id, peak_id] = peak[2]
            lil_matrix_state[graph_id, peak_id] = peak_type_dict[peak[3][0]]

            # exit()
    # log.debug('lil_matrix_data {}'.format(lil_matrix_data))
    csr_matrix_interaction_data =  lil_matrix_interaction_data.tocsr()
    csr_matrix_state =  lil_matrix_state.tocsr()

    resultFileH5Object.create_dataset("csr_matrix_interaction_data", data=csr_matrix_interaction_data.data, compression="gzip", compression_opts=9)
    resultFileH5Object.create_dataset("csr_matrix_interaction_indices", data=csr_matrix_interaction_data.indices, compression="gzip", compression_opts=9)
    resultFileH5Object.create_dataset("csr_matrix_interaction_indptr", data=csr_matrix_interaction_data.indptr, compression="gzip", compression_opts=9)

    resultFileH5Object.create_dataset("csr_matrix_state_data", data=csr_matrix_state.data, compression="gzip", compression_opts=9)
    resultFileH5Object.create_dataset("csr_matrix_state_indices", data=csr_matrix_state.indices, compression="gzip", compression_opts=9)
    resultFileH5Object.create_dataset("csr_matrix_state_indptr", data=csr_matrix_state.indptr, compression="gzip", compression_opts=9)


    resultFileH5Object.create_dataset('peak_types_list', data=list(peak_type_dict.keys()), compression="gzip", compression_opts=9 )

    resultFileH5Object.create_dataset('reference_point_list', data=list(reference_point_dict.keys()), compression="gzip", compression_opts=9 )
    resultFileH5Object.create_dataset('peak_list', data=list(peak_dict.keys()), compression="gzip", compression_opts=9 )
    resultFileH5Object.close()

def main(args=None):
    args = parse_arguments().parse_args(args)
    viewpointObj = Viewpoint()

    fileList = []
    # read hdf file
    fileHDF5Object = h5py.File(args.significantFile, 'r')

    fileType = fileHDF5Object.attrs['type']

    if fileType != 'significant':
        log.error('Please use a significant interactions file created by chicSignificantInteractions. Exiting.')
        exit(1)
    if fileHDF5Object.attrs['combinationMode'] == 'dual':
        log.error('Please use a significant interactions file created by chicSignificantInteractions with one matrix. Exiting.')
        exit(1)
    # read hdf file
    keys_file = list(fileHDF5Object.keys())
    for i, sample in enumerate(keys_file):
        matrix_obj1 = fileHDF5Object[sample]
        chromosomeList1 = sorted(list(matrix_obj1.keys()))
        chromosomeList1.remove('genes')
        for chromosome1 in chromosomeList1:
            geneList1 = sorted(list(matrix_obj1[chromosome1].keys()))
            for gene1 in geneList1:
                fileList.append([[sample, chromosome1, gene1]])

    fileHDF5Object.close()

    # read chromatin peaks file 
    peaks_chromatin_status_interval_tree, peaks_metadata_dict, peak_list, peak_types = readPeaksChromatinFile(args.peakFile)
    # log.debug(peak_list)

    # images_array, file_name_list, reference_point_graph = computeGraph(fileList, peaks_chromatin_status_interval_tree, peaks_metadata_dict, viewpointObj, args)

    
    # call graph computation

    filesPerThread = len(fileList) // args.threads

    all_data_collected = False
    images_array_thread = [None] * args.threads
    file_name_list_thread = [None] * args.threads
    reference_point_graph_thread = [None] * args.threads

    queue = [None] * args.threads
    process = [None] * args.threads
    thread_done = [False] * args.threads
    fail_flag = False
    fail_message = ''

    for i in range(args.threads):

        if i < args.threads - 1:
            fileListPerThread = fileList[i * filesPerThread:(i + 1) * filesPerThread]
        else:
            fileListPerThread = fileList[i * filesPerThread:]
        queue[i] = Queue()
# pFileList, pPeakDataIntervalTree, pPeaksMetadataDict, pViewpointObject, pArgs, pQueue
        process[i] = Process(target=computeGraph, kwargs=dict(
                                pFileList=fileList, 
                                pPeakDataIntervalTree=peaks_chromatin_status_interval_tree,
                                pPeaksMetadataDict=peaks_metadata_dict, 
                                pViewpointObject=viewpointObj,
                                pArgs=args,
                                pQueue=queue[i]
        )
        )

        process[i].start()

    while not all_data_collected:
        for i in range(args.threads):
            if queue[i] is not None and not queue[i].empty():
                return_content = queue[i].get()
                if 'Fail:' in return_content:
                    fail_flag = True
                    fail_message = return_content[6:]
                else:
                    images_array_thread[i], file_name_list_thread[i], reference_point_graph_thread[i] = return_content

                queue[i] = None
                process[i].join()
                process[i].terminate()
                process[i] = None
                thread_done[i] = True
        all_data_collected = True
        for thread in thread_done:
            if not thread:
                all_data_collected = False
        time.sleep(1)
    if fail_flag:
        log.error(fail_message)
        exit(1)

    images_array = [item for sublist in images_array_thread for item in sublist]
    file_name_list = [item for sublist in file_name_list_thread for item in sublist]
    reference_point_graph = {}

    # z = {**x, **y}
    for sublist in reference_point_graph_thread:
        reference_point_graph = {**reference_point_graph, **sublist}

    with tarfile.open(args.plotsFileName, "w:gz") as tar:
        for i, bufferObject in enumerate(images_array):
            # with closing(bufferObject) as fobj:
                # tar_info = tarfile.TarInfo(name=file_name_list[i] + '.' + html)
                # tar_info.mtime = time.time()
                # tar_info.size = len(fobj.getvalue())
                # fobj.seek(0)
                # tar.addfile(tarinfo=tar_info, fileobj=fobj)

            # string = StringIO.StringIO()
            # string.write("hello")
            # io.BytesIO(bufferObject)
            # bufferObject.seek(0)

            # info = tarfile.TarInfo(name='foo.txt')
            # info.size = len(bufferObject)
            info = tarfile.TarInfo(name=file_name_list[i] + '.' + "html")
            info.size=len(bufferObject)
            tar.addfile(tarinfo=info, fileobj=io.BytesIO(bufferObject))

        # tar.close()

    # hdf5 file design
    # matrix_name / chr / gene name
    #  - reference point: start end
    #  - connection list start: start ids
    #  - connection list end: end ids
    #  - classification list
    #  - interaction strength list
    #  - peak name list

    # resultFileH5Object = h5py.File(pOutFileName, 'w')
    # resultFileH5Object.attrs['type'] = "differential"
    # resultFileH5Object.attrs['version'] = __version__

    writeGraphHDF(args.outFileName, reference_point_graph, peak_list, peak_types)
